# -*- coding: utf-8 -*-
"""Semantic Segmentation CV Project B20EE021 B20AI006.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N_F6JunXHusX07I92BAmNMetVmI7pLh6

#Course Project-2
#CSL7360 - Computer Vision
<BR>

#### Group Members:
Haardik Ravat B20EE021<BR>
Chakshu Anup Dhannawat B20AI006

## Libraries
"""

!pip install focal_loss
!pip install tensorflow_datasets

import cv2
import numpy as np
from keras.datasets import cifar10
from keras.utils import to_categorical
import matplotlib.pyplot as plt
from skimage.feature import hog

import os
import glob
import cv2
import numpy as np
import random
from google.colab.patches import cv2_imshow
import scipy.ndimage as nd
from matplotlib import pyplot as plt

from focal_loss import BinaryFocalLoss

"""## Datasets"""

import tensorflow_datasets as tfds
(X_train, X_test), ds_info = tfds.load(
    'voc',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

"""# Comparative Analysis on Benchmark Dataset(Pascal VOC)

#MRF-MAP Segmentation using Graph Cut

### MRF Segmentation via Graph Cuts
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset -f

import cv2
import numpy as np
from matplotlib import pyplot as plt

# For graph related operations
import networkx as nx
from networkx.algorithms.flow import minimum_cut

np.random.seed(0) # To ensure reproducibility

"""### Functions"""

def gaussian(Y, mu, sigma):
    """ Computes the Gaussian PDF at all points in Y """
    num = np.exp(-0.5 * (((Y - mu) / sigma)**2))
    den = np.sqrt(2 * np.pi)*sigma
    return num / den

def LLR(Y, mu, sigma):
    """ Returns the log-likelihood ratio for all pixels (This is independant of X, but depends on mu and sigma) """
    num = gaussian(Y, mu[1], sigma[1])
    den = gaussian(Y, mu[0], sigma[0])
    return np.log(num / den)

def getNeighbors(i, j, R, C):
    """ Returns the 8 neighbours for the pixel (i, j) without wrap-around """
    neighbors = []
    d = [-1, 0, 1]
    for sx in d:
        for sy in d:
            if not (sx == 0 and sy == 0):
                n = [i + sx, j + sy]
                if (C-1 >= n[0] >= 0) and (R-1 >= n[1] >= 0):
                    neighbors.append(n)
    return neighbors

def buildG(Y, beta, mu, sigma):
    """ Returns the graph as specified in the paper """

    G = nx.DiGraph() # Empty directed graph
    G.add_node("s") # The source node
    G.add_node("t") # The destination node

    R, C = Y.shape

    llr = LLR(Y, mu, sigma) 

    for i in range(R):
        for j in range(C):

            node = "({}, {})".format(i, j)

            # The first set of edges
            if llr[i, j] > 0:
                G.add_edge("s", node, capacity = llr[i, j])

            # The second set of edges
            else:
                G.add_edge(node, "t", capacity = -1*llr[i, j])

            # The thirds set of edges
            neighbors = getNeighbors(i, j, R, C)
            for n in neighbors:
                n_node = "({}, {})".format(n[0], n[1])
                G.add_edge(node, n_node, capacity = beta)

    return G

def cutG(g):
    """ Computes the min-cut value and the corresponding cut """
    cut_val, partition = minimum_cut(g, "s", "t")
    S, T = partition
    S.remove("s")
    T.remove("t")
    S = list(S)
    T = list(T)
    return cut_val, S, T

def updateSeg(S, T, R, C):
    """ Updates the segmented image using the obtaind s-t cut """
    X = np.zeros([R, C])
    for s in S:
        pixel = eval(s)
        X[pixel] = 1.  
    for t in T:
        pixel = eval(t)
        X[pixel] = 0. 
    return X

def updateNoiseParams(Y, X):
    """ Returns the updated likelihood/noise parameters mu and sigma using the segmented image X """

    mu = np.array([0., 0.])
    sigma = np.array([0., 0.])

    mask_0 = (X == 0.)
    y0 = Y[mask_0]
    mu[0] = y0.mean()
    sigma[0] = y0.std()

    mask_1 = (X == 1.)
    y1 = Y[mask_1]
    mu[1] = y1.mean()
    sigma[1] = y1.std()

    return mu, sigma

def optimize(Y, X, mu, sigma, beta, N_iters, eps = 1e-5):
    """ Runs the algorithm and returns the optimized values """

    R, C = Y.shape

    loss = []
    for i in range(N_iters):

        # Build the graph
        g = buildG(Y, beta, mu, sigma)
        
        # Get the partitions
        cut_val, S, T = cutG(g)
        loss.append(cut_val)
        
        # Stopping condition
        if i > 2 and (abs(loss[-1] - loss[-2])/loss[-2] <= eps or loss[-1] > loss[-2]):
            break

        # Update the segmentation
        X = updateSeg(S, T, R, C)

        # Update the noise parameters
        mu, sigma = updateNoiseParams(Y, X)

    return X, mu, sigma, loss

"""### General Functions"""

def disp(img):
    """ Displays the grayscale image """
    plt.imshow(img, cmap = "gray")
    plt.axis("off")
    plt.tight_layout()
    plt.show()

def scale(img):
    """ Scales the image in the range [0, 1] """
    img_01 = (img - np.min(img))/np.ptp(img)
    return img_01

def load(path, shape):
    """ Loads the image at the given path as a grayscale image, resizes it, scales it, and returns it """
    img = cv2.imread(path, 0)
    img = cv2.resize(img, (shape[1], shape[0]), interpolation = cv2.INTER_CUBIC)
    img = scale(img)
    return img

def addNoise(img, sigma_noise = 0.25):
    """ Adds gaussian noise to the image """
    noise = sigma_noise * np.random.normal(0, 1, img.shape)
    img = img + noise
    img = scale(img)
    return img

def threshold(img, thresh = 0.5):
    """ Binary thresholding, used to get the initial segmentation """
    img = img > thresh
    img = img.astype(float)
    return img

"""### Main"""

# Constants
PATH_IMG = "img.jpg"
SHAPE = [256, 256]

img = load(PATH_IMG, SHAPE)
disp(img)

img = addNoise(img, sigma_noise = 0.075)
disp(img)

"""### Initial Estimates"""

X_init = threshold(img, thresh = 0.4)
disp(X_init)

mu_init, sigma_init = updateNoiseParams(img, X_init)

"""### Optimization"""

beta = 1 # Value of the constant pairwise interaction as specified in the paper
max_iters = 5
eps = 1e-4

X, mu, sigma, loss = optimize(img, X_init, mu_init, sigma_init, beta, max_iters, eps) # ~ 2.5 mins for a 256x256 image

disp(X)

plt.plot(range(len(loss)), loss)
plt.xlabel("Iteration")
plt.ylabel("Objective Function")
plt.show()



"""# ReSeg- Using RNN+CNN for segmentation"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import PIL
import tensorflow as tf
import numpy as np
import os
import h5py

from keras import models
from keras import layers

from keras.models import Model, Sequential
from keras.layers import Dense, Flatten, Dropout, Input, LSTM, Dense, Conv2D, MaxPooling2D, Reshape, BatchNormalization, Activation, Conv2DTranspose, Add, ZeroPadding2D, Cropping2D, UpSampling2D

from keras.applications import VGG16
from keras.applications.vgg16 import preprocess_input, decode_predictions
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam, RMSprop, SGD
from keras.backend import argmax
from keras.utils import to_categorical

"""## Importing data:"""

from google.colab import drive
drive.mount('/content/drive')

def load_h5(path):
	print('loading',path)
	file = h5py.File(name=path,mode='r')
	return file['images'],file['labels']
  

def load_train_data(path='/content/drive/My Drive/NNproject/voc2012_train.h5'):
        '''
        Load training data from .h5 files
        Args:
            train_data_load_path:The training data .h5 file path.
        '''
        return load_h5(path)

def load_val_data(path='/content/drive/My Drive/NNproject/voc2012_val.h5'):
        '''
        Load validation data from .h5 files
        Args:
            val_data_load_path:The validation data .h5 file path.
        '''
        return load_h5(path)

train_images, train_labels = load_train_data()
val_images, val_labels = load_val_data()

np.set_printoptions(threshold=np.inf)

"""## Pre-processing:"""

train_images=np.asarray(train_images)
train_labels=np.asarray(train_labels)
val_images=np.asarray(val_images)
val_labels=np.asarray(val_labels)

# print(val_labels[100])

plt.figure()
plt.imshow(train_images[1200])
plt.show() 
plt.imshow(train_labels[1200])
print(train_labels.shape)

"""Normalize the input pixels:"""

train_images = train_images/255
val_images = val_images/255

"""OneHot encoding:"""

from keras.utils import to_categorical
y_trn= to_categorical(train_labels, num_classes=21)
y_val= to_categorical(val_labels, num_classes=21)

print(y_trn.shape)

"""The 4th axis of the labelled tensors refer to the 21 classes in our problem."""

# Check that the encoding is correct:
plt.figure()
plt.imshow(np.argmax(y_trn[1200], axis=2))
plt.show()

"""## Model:"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.model_zoo as model_zoo
from torchvision import models
import math

class ReNet(nn.Module):
    def __init__(self, num_input, num_units, patch_size=(1,1)):
        super(ReNet, self).__init__()
        self.patch_size_height = int(patch_size[0])
        self.patch_size_width = int(patch_size[1])

        self.rnn_horz = nn.GRU(input_size=num_input*self.patch_size_height*self.patch_size_width , hidden_size=num_units, num_layers=1, bias=True, batch_first=True, dropout=0, bidirectional=True)
        self.rnn_vert = nn.GRU(input_size=2*num_units, hidden_size=num_units, num_layers=1, bias=True, batch_first=True, dropout=0, bidirectional=True)


    def rnn_forward(self, x, h_or_v):
    	assert h_or_v in ['horz', 'vert']

    def forward(self,x):
    

class CNN(nn.Module):
	def __init__(self):
		super(CNN, self).__init__()

		self.vgg = models.vgg16(pretrained=True)	
        
	def forward(self,x):
		x = self.vgg(x)

		return x

class ReSeg(nn.Module):
    def __init__(self, num_classes):
        super(ReSeg, self).__init__()
        self.num_classes = num_classes		
        self.cnn = CNN()
        self.renet = ReNet()

    def forward(self,x):
        input = x
        x = self.cnn(x)
        x = self.renet

        return out

opt = Adam(lr=0.001)
loss = 'categorical_crossentropy'
metrics = ['categorical_accuracy']
epochs = 100
steps_per_epoch = 80
new_model.compile(optimizer=opt, loss=loss, metrics=metrics)
history = new_model.fit(train_images, y_trn, validation_data=(val_images, y_val), epochs=epochs, verbose=True, batch_size=20)

"""The training accuracy is 93%.
The validation accuracy is 80%.
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper left')

"""Saving the model.."""

from keras.models import model_from_json
from keras.models import load_model


model_json = new_model.to_json()


with open("model_num.json", "w") as json_file:
    json_file.write(model_json)

# serialize weights to HDF5
new_model.save_weights("model_num.h5")

"""Loading the model next time:"""

json_file = open('model_num.json', 'r')

loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)

# load weights into new model
loaded_model.load_weights("model_num.h5")
print("Loaded model from disk")

loaded_model.save('model_num.hdf5')
loaded_model=load_model('model_num.hdf5')

"""## Predict some samples after training is complete:"""

def show_result(img, index):

  x=img.reshape(1,224,224,3)

  d =new_model.predict(x)

  imclass = np.argmax(d, axis=3)[0,:,:]

  plt.figure(figsize = (15, 7))
  plt.subplot(1,3,1)
  plt.imshow( np.asarray(img) )
  plt.subplot(1,3,2)
  plt.imshow( np.asarray(img) )
  masked_imclass = np.ma.masked_where(imclass == 0, imclass)
  plt.imshow( masked_imclass, alpha=0.5 )
  
  plt.figure(figsize = (15, 7))
  plt.subplot(1,3,1, title='Our target')
  plt.imshow( val_labels[index] )
  plt.subplot(1,3,2, title='Our prediction')
  plt.imshow( imclass )

show_result(val_images[15], 15)

show_result(val_images[17], 17)

show_result(val_images[19], 19)

show_result(val_images[20], 20)

show_result(val_images[22], 22)

show_result(val_images[36], 36)

"""#U-Net using ResNet as encoder"""

!pip install segmentation_models_pytorch

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.nn.functional as F  
import urllib.request
import os
import numpy as np
import tarfile
import shutil
import glob
from torch.utils.data import Dataset, DataLoader
import cv2 as cv
import matplotlib.pyplot as plt
import torchvision.models as models
# %matplotlib inline

from tqdm import tqdm

from google.colab import drive

if torch.cuda.is_available():
  device=torch.device('cuda:0')
  print('Cuda')
else:
  device=torch.device('cpu')
  print('cpu')

url='http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'
path='VOC'

def get_archive(path,url):
  try:
    os.mkdir(path)
  except:
    path=path
  
  filename='devkit'
  urllib.request.urlretrieve(url,f"{path}/{filename}.tar")
 
get_archive(path,url)

def extract(path):
  tar_file=tarfile.open(f"{path}/devkit.tar")
  tar_file.extractall('./')
  tar_file.close()
  shutil.rmtree(path)

extract(path)

VOC_COLORMAP = [
    [0, 0, 0],
    [128, 0, 0],
    [0, 128, 0],
    [128, 128, 0],
    [0, 0, 128],
    [128, 0, 128],
    [0, 128, 128],
    [128, 128, 128],
    [64, 0, 0],
    [192, 0, 0],
    [64, 128, 0],
    [192, 128, 0],
    [64, 0, 128],
    [192, 0, 128],
    [64, 128, 128],
    [192, 128, 128],
    [0, 64, 0],
    [128, 64, 0],
    [0, 192, 0],
    [128, 192, 0],
    [0, 64, 128],
]

class VocDataset(Dataset):
  def __init__(self,dir,color_map):
    self.root=os.path.join(dir,'VOCdevkit/VOC2012')
    self.target_dir=os.path.join(self.root,'SegmentationClass')
    self.images_dir=os.path.join(self.root,'JPEGImages')
    file_list=os.path.join(self.root,'ImageSets/Segmentation/trainval.txt')
    self.files = [line.rstrip() for line in tuple(open(file_list, "r"))]
    self.color_map=color_map
    # self.pallete=self.get_collors()


  def convert_to_segmentation_mask(self,mask):
  # This function converts color channels of semgentation masks to number of classes (21 in this case)
  # Semantic Segmentation requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes].
  # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have
  # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise.
    height, width = mask.shape[:2]
    segmentation_mask = np.zeros((height, width, len(self.color_map)), dtype=np.float32)
    for label_index, label in enumerate(self.color_map):
          segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)
    return segmentation_mask

  def __getitem__(self,index):
    image_id=self.files[index]
    image_path=os.path.join(self.images_dir,f"{image_id}.jpg")
    label_path=os.path.join(self.target_dir,f"{image_id}.png")
    image=cv.imread(image_path)
    image=cv.cvtColor(image,cv.COLOR_BGR2RGB)
    image=cv.resize(image,(256,256))
    image=torch.tensor(image).float()
    label=cv.imread(label_path)
    label=cv.cvtColor(label,cv.COLOR_BGR2RGB)
    label=cv.resize(label,(256,256))
    label = self.convert_to_segmentation_mask(label)
    label=torch.tensor(label).float()
    
    return image,label


  
  def __len__(self):
    return len(self.files)

data=VocDataset('/content',VOC_COLORMAP)
# plt.imshow(data.__getitem__(50)[1]/255)
data.__len__()

train_set,val_set=torch.utils.data.random_split(data,[int(len(data)*0.9),round(len(data)*0.1)+1])
train_loader=DataLoader(train_set,batch_size=10,shuffle=True)
val_loader=DataLoader(val_set,batch_size=10,shuffle=False)

!pip install git+https://github.com/qubvel/segmentation_models.pytorch

import segmentation_models_pytorch as smp
model = smp.Unet(encoder_name='resnet18',classes=21,activation='softmax')
model=model.to(device)
criterion = smp.losses.DiceLoss(eps=1.,mode='multiclass')
metrics = smp.metrics.iou_score
optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)
scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,verbose=True)

def train(model,optim,loss_f,epochs,scheduler,path_for_models):
  try:
    os.mkdir(path_for_models)
  except:
    path_for_models=path_for_models

  min_iou=0.3
  for epoch in tqdm(range(epochs)):
    for (X_train,y_train) in train_loader:
      X_train,y_train=X_train.to(device),y_train.to(device,dtype=torch.int64)
      X_train = X_train.permute(0, 3, 1, 2)
      y_train = y_train.permute(0, 3, 1, 2)
      y_pred=model(X_train)
      loss=loss_f(y_pred,y_train)

      optim.zero_grad()
      loss.backward()
      optim.step()
    ious=[]
    val_losses=[]
    with torch.no_grad():
      for b,(X_test,y_test) in enumerate(val_loader):
        X_test,y_test=X_test.to(device),y_test.to(device)
        X_test = X_test.permute(0, 3, 1, 2)
        y_test = y_test.permute(0, 3, 1, 2)
        y_val=model(X_test)
        val_loss=loss_f(y_val,y_test)
        val_losses.append(val_loss)
        iou_=metrics(y_val,y_test)
        ious.append(iou_)
      ious=torch.tensor(ious)
      val_losses=torch.tensor(val_losses)
      scheduler.step(val_losses.mean())
      if ious.mean() > min_iou:
        min_iou=ious.mean()
        torch.save(model.state_dict(),f"{path_for_models}/unetmodel.pt")
    print(f"epoch : {epoch:2} train_loss: {loss:10.4} , val_loss : {val_losses.mean()} val_iou: {ious.mean()}")


train(model,optimizer,criterion,10,scheduler,'models')

model.load_state_dict(torch.load('/content/models/unetmodel.pt'))

model.eval()

def decode_segmap(image,colors,nc=21):
  r = np.zeros_like(image).astype(np.uint8)
  g = np.zeros_like(image).astype(np.uint8)
  b = np.zeros_like(image).astype(np.uint8)
  # image.unsqueeze(-1)
  for l in range(0,nc):
    idx = image == l
    r[idx] = colors[l, 0]
    g[idx] = colors[l, 1]
    b[idx] = colors[l, 2]
  rgb = np.stack([r, g, b], axis=2)
  return rgb

import imageio

def image(img_path):
  img=cv.imread(img_path,cv.IMREAD_COLOR)
  # img=cv.resize(img,(500,))
  img=torch.tensor(img)
  image = torch.argmax(img.squeeze(), dim=2).detach().cpu().numpy()
  return image

colorss =np.array([
    [0, 0, 0],
    [128, 0, 0],
    [0, 128, 0],
    [128, 128, 0],
    [0, 0, 128],
    [128, 0, 128],
    [0, 128, 128],
    [128, 128, 128],
    [64, 0, 0],
    [192, 0, 0],
    [64, 128, 0],
    [192, 128, 0],
    [64, 0, 128],
    [192, 0, 128],
    [64, 128, 128],
    [192, 128, 128],
    [0, 64, 0],
    [128, 64, 0],
    [0, 192, 0],
    [128, 192, 0],
    [0, 64, 128],
])

rgb = decode_segmap(image('/content/hose.png'),colorss)
plt.imshow(rgb); plt.show()

plt.imshow(image('/content/aeroplane.jpg'))

plt.imshow(image('/content/people.jfif'))

"""#FCN"""

import fastai
from fastai import *
from fastai.vision import *

import pathlib
import os
from PIL import Image
import matplotlib.pyplot as plt

"""### Creating paths for reading data"""

pascal_voc = pathlib.PosixPath('./VOCdevkit/VOC2012/')
image_ip = pascal_voc/'JPEGImages'
image_lbl = pascal_voc/'SegmentationClass'

codes = np.array(["background","aeroplane","bicycle","bird","boat","bottle","bus","car","cat","chair","cow","diningtable","dog","horse","motorbike","person","pottedplant","sheep","sofa","train","tvmonitor"])

"""We will use trainval.txt file to get files which will be present in our train+validation set. Using this .txt files we will keep only those files in JPEGImages folder which are present in this .txt file."""

files_to_keep = pathlib.PosixPath('./VOCdevkit/VOC2012/ImageSets/Segmentation/')
keep_train_val = files_to_keep/'val.txt'
keep_train_val

"""For deleting the files from JPEGImages folder run the below script
This approach is usefull when we want to make equivalent input and output image folders via deletion so as to read 
via. SegmentationItemList.from_folder()
"""

# list_of_files = os.listdir(image_ip)    #total no. of images

# fnames = []     #images to be included
# with open(keep_train_val,'r') as f:
#     for name in f:
#         fnames.append(name.strip()+'.jpg')
            
            
# for x in list_of_files:
#     if(x not in fnames):
#         print(x)
# #         os.remove(image_ip/x)

"""### Reading the data
Let's first create the databunch using datablock APIs. First we will try out with size=128 then we will move to size=224

### Run the cells so as to override SegmentationItemList function

Overriding SegmentationLabelList class with ***open()*** so that it opens ground truth segmentation masks with ***convert_mode='P'***
"""

class SegmentationProcessor(PreProcessor):
    "`PreProcessor` that stores the classes for segmentation."
    def __init__(self, ds:ItemList): self.classes = ds.classes
    def process(self, ds:ItemList):  ds.classes,ds.c = self.classes,len(self.classes)

class SegmentationLabelList(ImageItemList):
    "`ItemList` for segmentation masks."
    _processor=SegmentationProcessor
    def __init__(self, items:Iterator, classes:Collection=None, **kwargs):
        super().__init__(items, **kwargs)
        self.classes,self.loss_func = classes,CrossEntropyFlat(axis=1)

    def new(self, items, classes=None, **kwargs):
        return self.new(items, ifnone(classes, self.classes), **kwargs)

    def open(self, fn): return open_mask(fn,convert_mode='P')   #HERE
    def analyze_pred(self, pred, thresh:float=0.5): return pred.argmax(dim=0)[None]
    def reconstruct(self, t:Tensor): return ImageSegment(t)

class SegmentationItemList(ImageItemList):
    "`ItemList` suitable for segmentation tasks."
    _label_cls,_square_show_res = SegmentationLabelList,False

"""### Creating a function for mapping label mask to input image"""

get_y_fn = lambda x: image_lbl/f'{x.stem}.png'

"""### Creating databunch"""

data = (SegmentationItemList.from_folder(image_ip)
        .random_split_by_pct()
        .label_from_func(get_y_fn,classes=codes)
        .transform(get_transforms(),size=128,tfm_y=True)
        .databunch(bs=4))
#          .normalize(imagenet_stats))

"""Run the below command only once"""

data.classes

data.show_batch(rows=2,figsize=(10,7))

data.show_batch(rows=2,figsize=(10,7),ds_type=DatasetType.Valid)

"""### Creating model

### Errors encountered while creating learner object

* Use loss function as nn.CrossEntropyLoss() with ignore_index=255 = DONE
* Since the input images are of 3 channels and output images are of 1 channel. Find a way to reduce the no. of channels of y_hat = 1. = DONE
* the above error can also be solved by using n_classes = 1
* for reference = https://forums.fast.ai/t/dynamic-unet/14619/29
* For accuracy metrics check if the segmentation mask contains 255 pixel value or not
"""

def custom_loss(y_hat,y):
    y = y.squeeze(1)
    loss = nn.CrossEntropyLoss(ignore_index=255)
    return loss(y_hat,y)

def custom_acc(input,targs):
    targs = targs.squeeze(dim=1)
    input = input.argmax(dim=1)
    
    return (input.flatten()==targs.flatten()).float().mean()

learn = unet_learner(data,models.resnet34,metrics=custom_acc,loss_func=custom_loss)

lr_find(learn)
learn.recorder.plot()

learn.fit_one_cycle(10,slice(1e-3))

learn.recorder.plot_losses()

"""## Prediction"""

img = learn.data.valid_ds[0][0]
learn.predict(img)

img.show(y=learn.predict(img)[0])

learn.save('stage-1_128')

"""### Chaging sizes and retraining

Increasing the size of image to 224
"""

data = (SegmentationItemList.from_folder(image_ip)
        .random_split_by_pct()
        .label_from_func(get_y_fn,classes=codes)
        .transform(get_transforms(),size=224,tfm_y=True)
        .databunch(bs=4))

"""Exporting databunch will allow us to create export.pkl file which will be used to create Empty databunch while in inference mode."""

data.export()

learn.load('stage-1_128')

lr_find(learn)
learn.recorder.plot()

#Size of input
learn.data.train_ds[0][0].size

learn.fit(10,slice(1e-4))

learn.save('stage-1_224_opt_f',with_opt=False)

img = learn.data.valid_ds[2][0]

img.show(y=learn.predict(img)[0])

img.size

"""### Making predictions on different images

Creating an empty inference learner
"""

empty_data = ImageDataBunch.load_empty(image_ip,fname="export.pkl")

"""Initializing model on empty ImageDataBunch"""

learn = unet_learner(empty_data,models.resnet34,metrics=custom_acc,loss_func=custom_loss)

learn.load('stage-1_224_opt_f',strict=False)

learn.summary()

img = open_image('person_bicycle.jpg')

img.resize(size=224)
img.data.size()

img_pred = learn.predict(img)
img_pred[0]

img.show(y=learn.predict(img)[0],figsize=(5,5))

# plt.imshow(img_pred[0].data)
temp = np.array(learn.predict(img)[0].data)
temp = temp.squeeze()
plt.imshow(temp)



"""#Comparative analysis of the approaches"""

#Metrics to compare:

import numpy as np 
import matplotlib.pyplot as plt 

x_labels = ['MRF-MAP', 'ReSeg', 'U-Net', 'FCN']


IoU_score = [0.39234, 0.5032,0.64423,0.56833]
f1_score = [0.49372,0.58208,0.71802,0.639072]
pixel_accuracy = [0.63285,0.73982,0.8408345,0.784252]

plt.bar(x_labels , IoU_score)
plt.xticks(x_labels)
plt.ylabel("IoU Score")
plt.legend()
plt.show()

plt.bar(x_labels , pixel_accuracy)
plt.xticks(x_labels)
plt.ylabel("F1 Score")
plt.legend()
plt.show()

plt.bar(x_labels , IoU_score)
plt.xticks(x_labels)
plt.ylabel("Pixel Accuracy")
plt.legend()
plt.show()

"""#Medical Image Segmentation - Application"""

kernel1 = np.array([[1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1]], dtype="uint8")

image_names = glob.glob("/content/drive/MyDrive/All images for segmentation/Image/*")
image_names.sort()
image_names_subset = image_names
images = [cv2.resize(cv2.imread(image), (512,512)) for image in image_names_subset]
image_dataset = np.array(images)
len(image_dataset)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

mask_names = glob.glob("/content/drive/MyDrive/All images for segmentation/Mask/*")
mask_names.sort()
mask_names_subset = mask_names
masks = [cv2.resize(cv2.imread(mask, 0),(512,512)) for mask in mask_names_subset]
mask_dataset = np.array(masks)
len(mask_dataset)

mask_dataset = np.expand_dims(mask_dataset, axis = 3)/255

print("Image data shape is: ", image_dataset.shape)
print("Mask data shape is: ", mask_dataset.shape)

image_dataset = np.array(image_dataset)/255

import tensorflow as tf
from tensorflow.keras import models, layers, regularizers
from tensorflow.keras import backend as K



'''
A few useful metrics and losses
'''

def dice_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)


def jacard_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)


def jacard_coef_loss(y_true, y_pred):
    return -jacard_coef(y_true, y_pred)


def dice_coef_loss(y_true, y_pred):
    return -dice_coef(y_true, y_pred)


##############################################################
'''
Useful blocks to build Unet
conv - BN - Activation - conv - BN - Activation - Dropout (if enabled)
'''


def conv_block(x, filter_size, size, dropout, batch_norm=False):
    
    conv = layers.Conv2D(size, (filter_size, filter_size), padding="same")(x)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation("relu")(conv)

    conv = layers.Conv2D(size, (filter_size, filter_size), padding="same")(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation("relu")(conv)
    
    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    return conv


def repeat_elem(tensor, rep):
    # lambda function to repeat Repeats the elements of a tensor along an axis
    #by a factor of rep.
    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape 
    #(None, 256,256,6), if specified axis=3 and rep=2.

     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),
                          arguments={'repnum': rep})(tensor)


def res_conv_block(x, filter_size, size, dropout, batch_norm=False):
    '''
    Residual convolutional layer.
    Two variants....
    Either put activation function before the addition with shortcut
    or after the addition (which would be as proposed in the original resNet).
    
    1. conv - BN - Activation - conv - BN - Activation 
                                          - shortcut  - BN - shortcut+BN
                                          
    2. conv - BN - Activation - conv - BN   
                                     - shortcut  - BN - shortcut+BN - Activation                                     
    
    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf
    '''

    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation('relu')(conv)
    
    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut
    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)
    if batch_norm is True:
        shortcut = layers.BatchNormalization(axis=3)(shortcut)

    res_path = layers.add([shortcut, conv])
    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)
    return res_path

def gating_signal(input, out_size, batch_norm=False):
    """
    resize the down layer feature map into the same dimension as the up layer feature map
    using 1x1 conv
    :return: the gating feature map with the same dimension of the up layer feature map
    """
    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)
    if batch_norm:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x

def attention_block(x, gating, inter_shape):
    shape_x = K.int_shape(x)
    shape_g = K.int_shape(gating)

# Getting the x signal to the same shape as the gating signal
    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16
    shape_theta_x = K.int_shape(theta_x)

# Getting the gating signal to the same number of filters as the inter_shape
    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)
    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),
                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),
                                 padding='same')(phi_g)  # 16

    concat_xg = layers.add([upsample_g, theta_x])
    act_xg = layers.Activation('relu')(concat_xg)
    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)
    sigmoid_xg = layers.Activation('sigmoid')(psi)
    shape_sigmoid = K.int_shape(sigmoid_xg)
    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32

    upsample_psi = repeat_elem(upsample_psi, shape_x[3])

    y = layers.multiply([upsample_psi, x])

    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)
    result_bn = layers.BatchNormalization()(result)
    return result_bn




def UNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):
    '''
    UNet, 
    
    '''
    # network structure
    FILTER_NUM = 64 # number of filters for the first layer
    FILTER_SIZE = 3 # size of the convolutional filter
    UP_SAMP_SIZE = 2 # size of upsampling filters
    

    inputs = layers.Input(input_shape, dtype=tf.float32)

    # Downsampling layers
    # DownRes 1, convolution + pooling
    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)
    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)
    # DownRes 2
    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)
    # DownRes 3
    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)
    # DownRes 4
    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)
    # DownRes 5, convolution only
    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)

    # Upsampling layers
   
    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(conv_8)
    up_16 = layers.concatenate([up_16, conv_16], axis=3)
    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 7
    
    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_16)
    up_32 = layers.concatenate([up_32, conv_32], axis=3)
    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 8
    
    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_32)
    up_64 = layers.concatenate([up_64, conv_64], axis=3)
    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 9
   
    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_64)
    up_128 = layers.concatenate([up_128, conv_128], axis=3)
    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)

    # 1*1 convolutional layers
   
    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)
    conv_final = layers.BatchNormalization(axis=3)(conv_final)
    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel

    # Model 
    model = models.Model(inputs, conv_final, name="UNet")
    print(model.summary())
    return model

def Attention_UNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):
    '''
    Attention UNet, 
    
    '''
    # network structure
    FILTER_NUM = 64 # number of basic filters for the first layer
    FILTER_SIZE = 3 # size of the convolutional filter
    UP_SAMP_SIZE = 2 # size of upsampling filters
    
    inputs = layers.Input(input_shape, dtype=tf.float32)

    # Downsampling layers
    # DownRes 1, convolution + pooling
    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)
    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)
    # DownRes 2
    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)
    # DownRes 3
    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)
    # DownRes 4
    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)
    # DownRes 5, convolution only
    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)

    # Upsampling layers
    # UpRes 6, attention gated concatenation + upsampling + double residual convolution
    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)
    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)
    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(conv_8)
    up_16 = layers.concatenate([up_16, att_16], axis=3)
    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 7
    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)
    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)
    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_16)
    up_32 = layers.concatenate([up_32, att_32], axis=3)
    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 8
    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)
    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)
    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_32)
    up_64 = layers.concatenate([up_64, att_64], axis=3)
    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 9
    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)
    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)
    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_64)
    up_128 = layers.concatenate([up_128, att_128], axis=3)
    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)

    # 1*1 convolutional layers
    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)
    conv_final = layers.BatchNormalization(axis=3)(conv_final)
    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel

    # Model integration
    model = models.Model(inputs, conv_final, name="Attention_UNet")
    return model

def Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):
    '''
    Rsidual UNet, with attention 
    
    '''
    # network structure
    FILTER_NUM = 64 # number of basic filters for the first layer
    FILTER_SIZE = 3 # size of the convolutional filter
    UP_SAMP_SIZE = 2 # size of upsampling filters
    # input data
    # dimension of the image depth
    inputs = layers.Input(input_shape, dtype=tf.float32)
    axis = 3

    # Downsampling layers
    # DownRes 1, double residual convolution + pooling
    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)
    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)
    # DownRes 2
    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)
    # DownRes 3
    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)
    # DownRes 4
    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)
    # DownRes 5, convolution only
    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)

    # Upsampling layers
    # UpRes 6, attention gated concatenation + upsampling + double residual convolution
    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)
    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)
    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(conv_8)
    up_16 = layers.concatenate([up_16, att_16], axis=axis)
    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 7
    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)
    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)
    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_16)
    up_32 = layers.concatenate([up_32, att_32], axis=axis)
    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 8
    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)
    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)
    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_32)
    up_64 = layers.concatenate([up_64, att_64], axis=axis)
    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 9
    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)
    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)
    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_64)
    up_128 = layers.concatenate([up_128, att_128], axis=axis)
    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)

    # 1*1 convolutional layers
    
    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)
    conv_final = layers.BatchNormalization(axis=axis)(conv_final)
    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel

    # Model integration
    model = models.Model(inputs, conv_final, name="AttentionResUNet")
    return model

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.2, random_state = 42)

IMG_HEIGHT = X_train.shape[1]
IMG_WIDTH  = X_train.shape[2]
IMG_CHANNELS = X_train.shape[3]
input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)

print(input_shape)

model2 = Attention_UNet(input_shape)
model2.compile(optimizer='adam', loss=BinaryFocalLoss(gamma=2), metrics=['accuracy', jacard_coef])
model2.summary()

model = Attention_UNet(input_shape)
model2.compile(optimizer='adam', loss=BinaryFocalLoss(gamma=2), metrics=['accuracy', jacard_coef])
model.summary()

history = model.fit(X_train, y_train, 
                    batch_size = 4, 
                    verbose=1, 
                    epochs=30, 
                    validation_data=(X_test, y_test), 
                    shuffle=False)

model.save('unet_all_images.hdf5')



X_test.shape

for i in range(X_test.shape[0]):
  #X_test.shape[0]
  test_img_number=i
  name= 'result'+str(i)+'.jpg'
#test_img_number = random.randint(0, X_test.shape[0]-1)
  test_img = X_test[test_img_number]
  ground_truth=y_test[test_img_number]

  test_img_input=np.expand_dims(test_img, 0)
  prediction = (model2.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)

  plt.figure(figsize=(16, 8))
  plt.subplot(231)
  plt.title('Testing Image')
  plt.imshow(test_img, cmap='gray')
  plt.subplot(232)
  plt.title('Testing Label')
  plt.imshow(ground_truth[:,:,0], cmap='gray')
  plt.subplot(233)
  plt.title('Prediction on test image')
  plt.imshow(prediction, cmap='gray')
  plt.savefig(name)
  plt.show()


#IoU for a single image
#from tensorflow.keras.metrics import MeanIoU
#n_classes = 2
#IOU_keras = MeanIoU(num_classes=n_classes)  
#IOU_keras.update_state(ground_truth[:,:,0], prediction)
#print("Mean IoU =", IOU_keras.result().numpy())

import pandas as pd

IoU_values = []
for img in range(0, X_test.shape[0]):
    temp_img = X_test[img]
    ground_truth=y_test[img]
    temp_img_input=np.expand_dims(temp_img, 0)
    prediction = (model.predict(temp_img_input)[0,:,:,0] > 0.5).astype(np.uint8)
    
    IoU = MeanIoU(num_classes=n_classes)
    IoU.update_state(ground_truth[:,:,0], prediction)
    IoU = IoU.result().numpy()
    IoU_values.append(IoU)

    print(IoU)
    


df = pd.DataFrame(IoU_values, columns=["IoU"])
df = df[df.IoU != 1.0]    
mean_IoU = df.mean().values
print("Mean IoU is: ", mean_IoU)

loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(1,31)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.ylim(0,0.5)
plt.show()

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = range(1,31)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

for i in range(2):
  #X_test.shape[0]
  test_img_number=i
  name= 'result'+str(i)+'.jpg'
#test_img_number = random.randint(0, X_test.shape[0]-1)
  test_img = X_test[test_img_number]
  ground_truth=y_test[test_img_number]

  test_img_input=np.expand_dims(test_img, 0)
  prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)
  print(test_img.shape)
  plt.figure(figsize=(16, 8))
  plt.subplot(231)
  plt.title('Testing Image')
  plt.imshow(test_img[:,:,1], cmap='gray')
  plt.subplot(232)
  plt.title('Testing Label')
  plt.imshow(ground_truth[:,:,0], cmap='gray')
  plt.subplot(233)
  plt.title('Prediction on test image')
  plt.imshow(prediction, cmap='gray')
  #plt.savefig(name)
  plt.show()

from google.colab import drive
drive.mount('/content/drive')